import axios from 'axios';
import * as cheerio from 'cheerio';
import { randomUUID } from 'crypto';
import { supabase } from '../database/supabase.js';

export interface ScrapeOptions {
    timeout: number;
    maxContentLength: number;
    followRedirects: boolean;
    cacheResults: boolean;
}

export interface ScrapedData {
    id: string;
    url: string;
    type: 'github' | 'website' | 'article' | 'documentation' | 'social' | 'video' | 'qa';
    title: string;
    description: string;
    content: string;
    metadata: {
        author?: string;
        language?: string;
        tags?: string[];
        projectFiles?: Record<string, any>;
        scrapingMethod?: string;
        publishDate?: string;
        readingTime?: string;
        claps?: string;
        platform?: string;
    };
    github?: {
        stars: number;
        forks: number;
        language: string;
        readme: string;
        topics: string[];
        projectFiles: Record<string, any>;
        size: number;
        defaultBranch: string;
        createdAt?: string;
        updatedAt?: string;
    };
    scrapedAt?: Date;
    status: 'success' | 'error';
}

export class LinkScrapingTools {
    private readonly defaultOptions: ScrapeOptions = {
        timeout: 15000,
        maxContentLength: 6000,
        followRedirects: true,
        cacheResults: true
    };

    /**
     * Main scraping method - detects link type and routes to appropriate scraper
     */
    public async scrapeLinks(
        urls: string[],
        userId: string,
        options: Partial<ScrapeOptions> = {}
    ): Promise<ScrapedData[]> {
        const finalOptions = { ...this.defaultOptions, ...options };
        const results: ScrapedData[] = [];

        console.log(`üîç Starting to scrape ${urls.length} link(s)...`);

        for (const url of urls) {
            try {
                console.log(`üîç Scraping: ${url}`);

                if (!this.isValidUrl(url)) {
                    throw new Error('Invalid URL format');
                }

                let scrapedData: ScrapedData;

                // Route to appropriate scraper based on URL
                if (url.includes('github.com')) {
                    scrapedData = await this.scrapeGitHub(url, finalOptions);
                } else if (url.includes('medium.com')) {
                    scrapedData = await this.scrapeMedium(url, finalOptions);
                } else if (url.includes('dev.to')) {
                    scrapedData = await this.scrapeDevTo(url, finalOptions);
                } else if (url.includes('hashnode.') || url.includes('.hashnode.dev')) {
                    scrapedData = await this.scrapeHashnode(url, finalOptions);
                } else if (url.includes('substack.com')) {
                    scrapedData = await this.scrapeSubstack(url, finalOptions);
                } else {
                    scrapedData = await this.scrapeWebsite(url, finalOptions);
                }

                // Cache results if enabled
                if (finalOptions.cacheResults) {
                    await this.cacheScrapedContent(scrapedData, userId);
                }

                results.push(scrapedData);
                console.log(`‚úÖ Successfully scraped: ${scrapedData.title}`);

            } catch (error) {
                console.error(`‚ùå Error scraping ${url}:`, error);
                results.push({
                    id: randomUUID(),
                    url,
                    type: 'website',
                    title: 'Error',
                    description: 'Failed to scrape content',
                    content: '',
                    metadata: {},
                    status: 'error'
                });
            }
        }

        return results;
    }

    /**
     * Scrape GitHub repository - Using WORKING logic from test-scraping.js
     */
    private async scrapeGitHub(url: string, options: ScrapeOptions): Promise<ScrapedData> {
        console.log(`üêô Scraping GitHub repository: ${url}`);

        try {
            // Extract owner and repo from URL (matching test-scraping.js)
            const match = url.match(/github\.com\/([^\/]+)\/([^\/]+)/);
            if (!match) {
                throw new Error('Invalid GitHub URL format');
            }

            const [, owner, repo] = match;
            console.log(`üìÇ Repository: ${owner}/${repo}`);

            // Step 1: Test main repository page scraping (exact logic from test-scraping.js)
            console.log('üìÇ Step 1: Testing repository main page scraping...');
            const mainPageResponse = await axios.get(url, {
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                },
                timeout: options.timeout
            });

            const $ = cheerio.load(mainPageResponse.data);

            // Extract repository metadata (exact selectors from test-scraping.js)
            const repoName = $('h1[data-pjax="#js-repo-pjax-container"] strong a').text().trim() || repo;
            const description = $('p.f4.my-3').text().trim() ||
                $('[data-pjax="#js-repo-pjax-container"] p').first().text().trim() ||
                'No description available';

            console.log('‚úÖ Repository metadata extracted:');
            console.log(`   üìõ Name: ${repoName}`);
            console.log(`   üìù Description: ${description}`);
            console.log(`   üîó URL: ${url}`);
            console.log(`   üõ†Ô∏è Method: Direct HTML scraping with Cheerio`);

            // Step 2: Testing README scraping (exact logic from test-scraping.js)
            console.log('\nüìñ Step 2: Testing README scraping...');
            let readmeContent = '';
            try {
                const readmeUrl = `https://raw.githubusercontent.com/${owner}/${repo}/main/README.md`;
                const readmeResponse = await axios.get(readmeUrl, { timeout: 10000 });
                readmeContent = readmeResponse.data;

                console.log('‚úÖ README scraped successfully:');
                console.log(`   üìÑ Length: ${readmeContent.length} characters`);
                console.log(`   üîó Source: ${readmeUrl}`);
                console.log(`   üõ†Ô∏è Method: Direct raw file access`);
                console.log(`   üìù Preview: ${readmeContent.substring(0, 300)}...`);
            } catch (readmeError) {
                console.log('‚ö†Ô∏è README scraping failed, trying master branch...');
                try {
                    const readmeUrlMaster = `https://raw.githubusercontent.com/${owner}/${repo}/master/README.md`;
                    const readmeResponse = await axios.get(readmeUrlMaster, { timeout: 10000 });
                    readmeContent = readmeResponse.data;

                    console.log('‚úÖ README scraped from master branch:');
                    console.log(`   üìÑ Length: ${readmeContent.length} characters`);
                    console.log(`   üìù Preview: ${readmeContent.substring(0, 200)}...`);
                } catch (masterError) {
                    console.log('‚ùå README not found in main or master branch');
                }
            }

            // Step 3: Testing project files scraping (exact logic from test-scraping.js)
            console.log('\nüì¶ Step 3: Testing project files scraping...');
            const projectFiles = await this.scrapeProjectFilesWithWorkingLogic(owner, repo);

            // Step 4: Summary (matching test-scraping.js output)
            console.log('\nüéâ Scraping test completed!');
            console.log(`üìã Summary:`);
            console.log(`   üìÇ Repository: ${owner}/${repo}`);
            console.log(`   üìÑ Project files found: ${Object.keys(projectFiles).length}`);
            console.log(`   üìù Files: ${Object.keys(projectFiles).join(', ')}`);

            // Step 5: Build result with working data structure
            const scrapedResult: ScrapedData = {
                id: randomUUID(),
                url,
                type: 'github',
                title: `${owner}/${repoName}`,
                description,
                content: readmeContent,
                metadata: {
                    author: owner,
                    language: 'JavaScript', // Would be extracted from page
                    tags: [],
                    projectFiles: projectFiles,
                    scrapingMethod: 'direct-web-scraping'
                },
                github: {
                    stars: 0, // Would be extracted from page
                    forks: 0, // Would be extracted from page
                    language: 'JavaScript',
                    readme: readmeContent,
                    topics: [],
                    projectFiles: projectFiles,
                    size: 0,
                    defaultBranch: 'main'
                },
                status: 'success'
            };

            console.log('\nüéØ Test Result: FREE GitHub scraping is working! ‚úÖ');
            console.log('üîó The scraper can extract:');
            console.log('   ‚úÖ Repository metadata');
            console.log('   ‚úÖ README content');
            console.log('   ‚úÖ Project configuration files');
            console.log('   ‚úÖ File contents for AI processing');

            return scrapedResult;

        } catch (error: any) {
            console.error('‚ùå Free GitHub scraping failed:', error instanceof Error ? error.message : 'Unknown error');
            throw new Error(`Failed to scrape GitHub repository directly: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    }

    /**
     * Scrape project files using EXACT working logic from test-scraping.js
     */
    private async scrapeProjectFilesWithWorkingLogic(owner: string, repo: string): Promise<Record<string, any>> {
        const filesToTest = [
            { name: 'package.json', type: 'json', description: 'Node.js dependencies and scripts' },
            { name: 'requirements.txt', type: 'text', description: 'Python dependencies' },
            { name: 'pyproject.toml', type: 'toml', description: 'Python project configuration' },
            { name: 'Cargo.toml', type: 'toml', description: 'Rust project configuration' },
            { name: 'go.mod', type: 'text', description: 'Go module configuration' },
            { name: 'LICENSE', type: 'text', description: 'Project license' },
            { name: 'Dockerfile', type: 'docker', description: 'Docker configuration' }
        ];

        const foundFiles = [];

        for (const file of filesToTest) {
            try {
                console.log(`üìÑ Checking for ${file.name}...`);

                // Try main branch first, then master (exact logic from test-scraping.js)
                const branches = ['main', 'master'];
                let content = '';
                let success = false;

                for (const branch of branches) {
                    try {
                        const rawUrl = `https://raw.githubusercontent.com/${owner}/${repo}/${branch}/${file.name}`;
                        const response = await axios.get(rawUrl, {
                            timeout: 8000,
                            headers: {
                                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                            }
                        });

                        content = typeof response.data === 'string' ? response.data : JSON.stringify(response.data);
                        success = true;
                        console.log(`‚úÖ Found ${file.name} (${content.length} chars)`);

                        // Parse JSON files for detailed info (exact logic from test-scraping.js)
                        if (file.type === 'json' && file.name === 'package.json') {
                            try {
                                const parsed = JSON.parse(content);
                                console.log(`   üì¶ Package details:`, {
                                    name: parsed.name,
                                    version: parsed.version,
                                    description: parsed.description,
                                    dependencies: Object.keys(parsed.dependencies || {}),
                                    devDependencies: Object.keys(parsed.devDependencies || {}),
                                    scripts: Object.keys(parsed.scripts || {})
                                });
                            } catch (parseError) {
                                console.log(`   ‚ö†Ô∏è Could not parse ${file.name} as JSON`);
                            }
                        }

                        foundFiles.push({
                            name: file.name,
                            type: file.type,
                            description: file.description,
                            content: content.substring(0, 500), // First 500 chars for preview
                            size: content.length
                        });

                        break;
                    } catch (branchError) {
                        // Try next branch
                        continue;
                    }
                }

                if (!success) {
                    console.log(`   ‚ùå ${file.name} not found`);
                }
            } catch (error) {
                console.log(`   ‚ùå Error checking ${file.name}:`, error instanceof Error ? error.message : 'Unknown error');
            }
        }

        // Convert to the expected format
        const projectFiles: Record<string, any> = {};
        foundFiles.forEach(file => {
            projectFiles[file.name] = file;
        });

        return projectFiles;
    }

    /**
     * Scrape Medium articles with specialized selectors
     */
    private async scrapeMedium(url: string, options: ScrapeOptions): Promise<ScrapedData> {
        try {
            const response = await axios.get(url, {
                timeout: options.timeout,
                maxRedirects: options.followRedirects ? 5 : 0,
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
            });

            const $ = cheerio.load(response.data);

            // Medium-specific selectors
            const title = $('h1[data-testid="storyTitle"]').text().trim() ||
                $('h1').first().text().trim() ||
                $('title').text().replace(' | Medium', '').trim() ||
                'No title found';

            const description = $('meta[name="description"]').attr('content') ||
                $('meta[property="og:description"]').attr('content') ||
                $('h2[data-testid="subtitle"]').text().trim() ||
                'No description available';

            // Extract author information
            const author = $('meta[name="author"]').attr('content') ||
                $('a[rel="author"]').text().trim() ||
                $('[data-testid="authorName"]').text().trim() ||
                $('.author-name').text().trim() ||
                $('span[data-testid="authorName"]').text().trim();

            // Extract publication date
            const publishDate = $('meta[property="article:published_time"]').attr('content') ||
                $('time').attr('datetime') ||
                $('[data-testid="storyPublishDate"]').text().trim();

            // Extract reading time
            const readingTime = $('span[data-testid="storyReadTime"]').text().trim() ||
                $('.reading-time').text().trim();

            // Extract main content with Medium-specific selectors
            let content = '';

            // Try Medium's article content selectors in order of preference
            const mediumContentSelectors = [
                'article section',
                '[data-testid="storyContent"]',
                '.postArticle-content',
                '.section-content',
                'article',
                '.post-content'
            ];

            for (const selector of mediumContentSelectors) {
                const element = $(selector);
                if (element.length > 0) {
                    // Extract text from paragraphs and headers, preserving structure
                    content = element.find('p, h1, h2, h3, h4, h5, h6, blockquote, li')
                        .map((_, el) => $(el).text().trim())
                        .get()
                        .filter(text => text.length > 0)
                        .join('\n\n');

                    if (content.length > 100) { // Only use if we got substantial content
                        break;
                    }
                }
            }

            // Fallback to body text if no content found
            if (!content || content.length < 100) {
                content = $('body').text().trim();
            }

            // Clean and limit content
            content = this.cleanContent(content);
            if (content.length > options.maxContentLength) {
                content = content.substring(0, options.maxContentLength) + '...';
            }

            // Extract tags/topics
            const tags = $('a[href*="/tag/"]').map((_, el) => $(el).text().trim()).get().slice(0, 5);

            // Extract claps/engagement if available
            const claps = $('[data-testid="clapCount"]').text().trim() ||
                $('.clapCount').text().trim();

            return {
                id: randomUUID(),
                url,
                type: 'article',
                title,
                description,
                content,
                metadata: {
                    author: author || undefined,
                    publishDate: publishDate || undefined,
                    readingTime: readingTime || undefined,
                    tags: tags.length > 0 ? tags : undefined,
                    claps: claps || undefined,
                    platform: 'Medium',
                    language: $('html').attr('lang') || 'en'
                },
                scrapedAt: new Date(),
                status: 'success'
            };
        } catch (error) {
            console.error('Error scraping Medium article:', error);
            throw new Error(`Failed to scrape Medium article: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    }

    /**
     * Scrape Dev.to articles with specialized selectors
     */
    private async scrapeDevTo(url: string, options: ScrapeOptions): Promise<ScrapedData> {
        try {
            const response = await axios.get(url, {
                timeout: options.timeout,
                maxRedirects: options.followRedirects ? 5 : 0,
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
            });

            const $ = cheerio.load(response.data);

            const title = $('h1.crayons-article__header__title').text().trim() ||
                $('h1').first().text().trim() ||
                'No title found';

            const description = $('meta[name="description"]').attr('content') ||
                $('meta[property="og:description"]').attr('content') ||
                'No description available';

            const author = $('.crayons-article__header__meta a').first().text().trim() ||
                $('meta[name="author"]').attr('content');

            const publishDate = $('time').attr('datetime');
            const readingTime = $('.crayons-article__header__meta time').next().text().trim();

            // Extract main content
            let content = $('.crayons-article__main .crayons-article__body').text().trim() ||
                $('#article-body').text().trim() ||
                $('article').text().trim();

            content = this.cleanContent(content);
            if (content.length > options.maxContentLength) {
                content = content.substring(0, options.maxContentLength) + '...';
            }

            // Extract tags
            const tags = $('.crayons-tag').map((_, el) => $(el).text().trim().replace('#', '')).get();

            return {
                id: randomUUID(),
                url,
                type: 'article',
                title,
                description,
                content,
                metadata: {
                    author: author || undefined,
                    publishDate: publishDate || undefined,
                    readingTime: readingTime || undefined,
                    tags: tags.length > 0 ? tags : undefined,
                    platform: 'Dev.to',
                    language: $('html').attr('lang') || 'en'
                },
                scrapedAt: new Date(),
                status: 'success'
            };
        } catch (error) {
            console.error('Error scraping Dev.to article:', error);
            throw new Error(`Failed to scrape Dev.to article: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    }

    /**
     * Scrape Hashnode articles with specialized selectors
     */
    private async scrapeHashnode(url: string, options: ScrapeOptions): Promise<ScrapedData> {
        try {
            const response = await axios.get(url, {
                timeout: options.timeout,
                maxRedirects: options.followRedirects ? 5 : 0,
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
            });

            const $ = cheerio.load(response.data);

            const title = $('h1.blog-title').text().trim() ||
                $('h1').first().text().trim() ||
                'No title found';

            const description = $('meta[name="description"]').attr('content') ||
                $('meta[property="og:description"]').attr('content') ||
                'No description available';

            const author = $('.author-name').text().trim() ||
                $('meta[name="author"]').attr('content');

            const publishDate = $('time').attr('datetime') ||
                $('.publish-date').text().trim();

            // Extract main content
            let content = $('.blog-content-wrapper').text().trim() ||
                $('.post-content').text().trim() ||
                $('article').text().trim();

            content = this.cleanContent(content);
            if (content.length > options.maxContentLength) {
                content = content.substring(0, options.maxContentLength) + '...';
            }

            // Extract tags
            const tags = $('.tag').map((_, el) => $(el).text().trim()).get();

            return {
                id: randomUUID(),
                url,
                type: 'article',
                title,
                description,
                content,
                metadata: {
                    author: author || undefined,
                    publishDate: publishDate || undefined,
                    tags: tags.length > 0 ? tags : undefined,
                    platform: 'Hashnode',
                    language: $('html').attr('lang') || 'en'
                },
                scrapedAt: new Date(),
                status: 'success'
            };
        } catch (error) {
            console.error('Error scraping Hashnode article:', error);
            throw new Error(`Failed to scrape Hashnode article: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    }

    /**
     * Scrape Substack newsletters with specialized selectors
     */
    private async scrapeSubstack(url: string, options: ScrapeOptions): Promise<ScrapedData> {
        try {
            const response = await axios.get(url, {
                timeout: options.timeout,
                maxRedirects: options.followRedirects ? 5 : 0,
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
            });

            const $ = cheerio.load(response.data);

            const title = $('.post-title').text().trim() ||
                $('h1.post-title').text().trim() ||
                $('h1').first().text().trim() ||
                'No title found';

            const description = $('meta[name="description"]').attr('content') ||
                $('meta[property="og:description"]').attr('content') ||
                $('.subtitle').text().trim() ||
                'No description available';

            const author = $('.byline-names').text().trim() ||
                $('.author-name').text().trim() ||
                $('meta[name="author"]').attr('content');

            const publishDate = $('time').attr('datetime') ||
                $('.post-date').text().trim();

            // Extract main content
            let content = $('.available-content').text().trim() ||
                $('.body').text().trim() ||
                $('.post-content').text().trim() ||
                $('article').text().trim();

            content = this.cleanContent(content);
            if (content.length > options.maxContentLength) {
                content = content.substring(0, options.maxContentLength) + '...';
            }

            return {
                id: randomUUID(),
                url,
                type: 'article',
                title,
                description,
                content,
                metadata: {
                    author: author || undefined,
                    publishDate: publishDate || undefined,
                    platform: 'Substack',
                    language: $('html').attr('lang') || 'en'
                },
                scrapedAt: new Date(),
                status: 'success'
            };
        } catch (error) {
            console.error('Error scraping Substack article:', error);
            throw new Error(`Failed to scrape Substack article: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    }

    /**
     * Scrape general website
     */
    private async scrapeWebsite(url: string, options: ScrapeOptions): Promise<ScrapedData> {
        try {
            const response = await axios.get(url, {
                timeout: options.timeout,
                maxRedirects: options.followRedirects ? 5 : 0,
                headers: {
                    'User-Agent': 'PostWizz-LinkScraper/1.0 (Mozilla/5.0 compatible)',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive'
                }
            });

            const $ = cheerio.load(response.data);

            // Extract basic information
            const title = $('title').text().trim() ||
                $('h1').first().text().trim() ||
                'No title found';

            const description = $('meta[name="description"]').attr('content') ||
                $('meta[property="og:description"]').attr('content') ||
                $('p').first().text().trim().substring(0, 200) ||
                'No description available';

            // Extract main content with improved selectors
            let content = '';

            // Enhanced content selectors for better extraction
            const contentSelectors = [
                // Semantic HTML5 selectors
                'article',
                'main article',
                '[role="main"] article',

                // Common blog/article selectors
                '.post-content',
                '.entry-content',
                '.article-content',
                '.content-body',
                '.story-content',

                // Generic content containers
                '.content',
                '.main-content',
                '#content',
                '#main-content',

                // Fallback to main sections
                'main',
                '.main',
                '#main',

                // Last resort
                'body'
            ];

            for (const selector of contentSelectors) {
                const element = $(selector);
                if (element.length > 0) {
                    // Remove unwanted elements before extracting text
                    element.find('script, style, nav, header, footer, aside, .sidebar, .navigation, .menu, .ads, .advertisement').remove();

                    // Extract text from meaningful elements, preserving structure
                    const textContent = element.find('p, h1, h2, h3, h4, h5, h6, blockquote, li, div')
                        .map((_, el) => {
                            const text = $(el).text().trim();
                            // Only include substantial text blocks
                            return text.length > 20 ? text : '';
                        })
                        .get()
                        .filter(text => text.length > 0)
                        .join('\n\n');

                    if (textContent.length > 200) { // Only use if we got substantial content
                        content = textContent;
                        break;
                    }
                }
            }

            // Enhanced fallback extraction
            if (!content || content.length < 200) {
                // Try to extract from paragraphs directly
                const paragraphs = $('p').map((_, el) => $(el).text().trim()).get()
                    .filter(text => text.length > 30)
                    .join('\n\n');

                if (paragraphs.length > content.length) {
                    content = paragraphs;
                } else {
                    // Last resort: clean body text
                    const bodyClone = $('body').clone();
                    bodyClone.find('script, style, nav, header, footer, aside, .sidebar, .navigation, .menu').remove();
                    content = bodyClone.text().trim();
                }
            }

            // Clean and limit content
            content = this.cleanContent(content);
            if (content.length > options.maxContentLength) {
                content = content.substring(0, options.maxContentLength) + '...';
            }

            // Extract metadata
            const author = $('meta[name="author"]').attr('content') ||
                $('meta[property="article:author"]').attr('content') ||
                $('.author').first().text().trim();

            return {
                id: randomUUID(),
                url,
                type: 'website',
                title,
                description,
                content,
                metadata: {
                    author: author || undefined,
                    language: $('html').attr('lang') || 'en'
                },
                scrapedAt: new Date(),
                status: 'success'
            };
        } catch (error) {
            console.error('Error scraping website:', error);
            throw new Error(`Failed to scrape website: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    }

    /**
     * Extract number from text (for stars, forks, etc.)
     */
    private extractNumberFromText(text: string): number {
        if (!text) return 0;
        const match = text.match(/[\d,]+/);
        if (match) {
            return parseInt(match[0].replace(/,/g, ''), 10) || 0;
        }
        return 0;
    }

    /**
     * Validate URL format
     */
    private isValidUrl(url: string): boolean {
        try {
            new URL(url);
            return true;
        } catch {
            return false;
        }
    }

    /**
     * Detect URL type for better routing
     */
    private detectUrlType(url: string): string {
        const lowerUrl = url.toLowerCase();

        if (lowerUrl.includes('github.com')) return 'github';
        if (lowerUrl.includes('medium.com')) return 'article';
        if (lowerUrl.includes('dev.to')) return 'article';
        if (lowerUrl.includes('hashnode.') || lowerUrl.includes('.hashnode.dev')) return 'article';
        if (lowerUrl.includes('substack.com')) return 'article';
        if (lowerUrl.includes('blog.') || lowerUrl.includes('/blog/')) return 'article';
        if (lowerUrl.includes('docs.') || lowerUrl.includes('documentation.')) return 'documentation';
        if (lowerUrl.includes('linkedin.com') || lowerUrl.includes('twitter.com')) return 'social';
        if (lowerUrl.includes('youtube.com') || lowerUrl.includes('youtu.be')) return 'video';
        if (lowerUrl.includes('stackoverflow.com') || lowerUrl.includes('stackexchange.com')) return 'qa';

        return 'website';
    }

    /**
     * Clean scraped content with enhanced processing
     */
    private cleanContent(content: string): string {
        return content
            // Remove excessive whitespace but preserve paragraph breaks
            .replace(/[ \t]+/g, ' ')  // Replace multiple spaces/tabs with single space
            .replace(/\n\s*\n\s*\n+/g, '\n\n')  // Replace multiple newlines with double newline
            .replace(/^\s+|\s+$/gm, '')  // Trim each line
            // Remove common unwanted patterns
            .replace(/\b(Share|Like|Follow|Subscribe|Sign up|Log in|Cookie|Privacy Policy|Terms of Service)\b/gi, '')
            .replace(/\b\d+\s+(min read|minute read|minutes read)\b/gi, '')
            .replace(/\b(Published|Updated|Posted)\s+on\s+\w+\s+\d+,?\s+\d{4}\b/gi, '')
            // Remove social media noise
            .replace(/\b\d+\s+(likes?|shares?|comments?|views?|claps?)\b/gi, '')
            .replace(/\b(Twitter|Facebook|LinkedIn|Instagram|YouTube)\s+(Follow|Like|Share)\b/gi, '')
            // Clean up remaining artifacts
            .replace(/\n{3,}/g, '\n\n')  // Limit to max 2 consecutive newlines
            .replace(/\s+([.!?])/g, '$1')  // Remove space before punctuation
            .trim();
    }

    /**
     * Cache scraped content in database
     */
    private async cacheScrapedContent(scrapedData: ScrapedData, userId: string): Promise<void> {
        try {
            // Create URL hash for caching
            const urlHash = Buffer.from(scrapedData.url).toString('base64').substring(0, 64);

            // Cache for 24 hours
            const expiresAt = new Date();
            expiresAt.setHours(expiresAt.getHours() + 24);

            const { error } = await supabase
                .from('scraped_content')
                .upsert({
                    id: scrapedData.id,
                    user_id: userId,
                    url: scrapedData.url,
                    type: scrapedData.type,
                    title: scrapedData.title,
                    content: scrapedData.content,
                    metadata: scrapedData.metadata,
                    scraped_at: scrapedData.scrapedAt,
                    status: scrapedData.status
                });

            if (error) {
                console.error('Error caching scraped content:', error);
            }

            // Also update the link cache
            await supabase
                .from('link_cache')
                .upsert({
                    url_hash: urlHash,
                    url: scrapedData.url,
                    scraped_data: scrapedData,
                    expires_at: expiresAt
                });
        } catch (error) {
            console.error('Error caching scraped content:', error);
            // Don't throw error - caching failure shouldn't break scraping
        }
    }

    /**
     * Detect links in text content (for MCP integration)
     */
    public async detectLinks({ text, userId }: { text: string, userId: string }): Promise<{ content: Array<{ type: string, text: string }> }> {
        try {
            const urlRegex = /https?:\/\/[^\s]+/g;
            const urls = text.match(urlRegex) || [];

            const links = urls.map(url => ({
                url,
                type: this.detectUrlType(url),
                position: text.indexOf(url)
            }));

            return {
                content: [{
                    type: "text",
                    text: JSON.stringify({
                        links,
                        totalFound: links.length,
                        text: text
                    })
                }]
            };
        } catch (error: any) {
            console.error('Error detecting links:', error);
            return {
                content: [{
                    type: "text",
                    text: JSON.stringify({
                        links: [],
                        totalFound: 0,
                        error: error instanceof Error ? error.message : 'Unknown error'
                    })
                }]
            };
        }
    }

    /**
     * Scrape multiple URLs (for MCP integration)
     */
    public async scrapeUrls({
        urls,
        userId,
        options = {}
    }: {
        urls: string[],
        userId: string,
        options?: Partial<ScrapeOptions>
    }): Promise<{ content: Array<{ type: string, text: string }> }> {
        try {
            const results = await this.scrapeLinks(urls, userId, options);

            const summary = {
                total: results.length,
                successful: results.filter(r => r.status === 'success').length,
                failed: results.filter(r => r.status === 'error').length
            };

            return {
                content: [{
                    type: "text",
                    text: JSON.stringify({
                        results,
                        summary,
                        scrapedAt: new Date().toISOString()
                    })
                }]
            };
        } catch (error: any) {
            console.error('Error scraping URLs:', error);
            return {
                content: [{
                    type: "text",
                    text: JSON.stringify({
                        results: [],
                        summary: { total: 0, successful: 0, failed: urls.length },
                        error: error instanceof Error ? error.message : 'Unknown error'
                    })
                }]
            };
        }
    }
}